{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "The best results in machine translation are still achieved by a good and big dataset with aligned sentences in source and target language.\n",
    "\n",
    "From [Tatoabe](https://tatoeba.org/) you can download already aligned sentences in German and Low German which were created by the community. As the dataset contains a relatively small amount of ~17.000 sentences and moreover with different spelling and grammar, it is just a first step for building a databasis.\n",
    "\n",
    "Moreover there is [Low German Wikipedia](https://nds.wikipedia.org/wiki/Wikipedia:H%C3%B6%C3%B6ftsiet) with over 60.000 articles (April 2020). An idea would be to align suitable sentences with the German wikipedia. Luckily Facebook's research team did this already for [all Wikipedial languages](https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix) including as a side product Low German-German. A first look into that datasets shows that there are many mismatched sentences. Moreover there is still the same problem of different spelling and grammar. Still this could be a good approach to extend the databasis.\n",
    "\n",
    "\n",
    "The goal of this notebook is to get clean datasets for training the different translation models. To achieve that we have to correct as much as possible the spelling and have to delete mismatched sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:04.537281Z",
     "start_time": "2020-04-23T14:08:04.534229Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T15:15:14.123428Z",
     "start_time": "2020-04-18T15:15:14.119291Z"
    }
   },
   "source": [
    "## Tatoabe dataset\n",
    "\n",
    "As a first step we load in the dataset from Tatoabe. You can download the tsv files from the website. Moreover you will need the \"links.csv\" from the website to match the sentences from German and Low German by their sentence id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:09.023225Z",
     "start_time": "2020-04-23T14:08:04.542884Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating column names for the tables\n",
    "column_names_platt = [\"id\", \"language\", \"nds\"]\n",
    "column_names_deu = [\"id\", \"language\", \"deu\"]\n",
    "\n",
    "nds_sentences = pd.read_csv(\"nds_sentences.tsv\", sep= \"\\t\", header = None, names=column_names_platt)\n",
    "deu_sentences = pd.read_csv(\"deu_sentences.tsv\", sep= \"\\t\", header = None, names=column_names_deu)\n",
    "link_sentences = pd.read_csv(\"links.csv\", sep= \"\\t\", header = None, names=[\"origin\",\"translation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:09.040664Z",
     "start_time": "2020-04-23T14:08:09.025150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>nds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8193</th>\n",
       "      <td>906561</td>\n",
       "      <td>nds</td>\n",
       "      <td>Uns Vörfohren kemen vör 150 Johr in dit Land.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>810834</td>\n",
       "      <td>nds</td>\n",
       "      <td>Se verlöövt uns nich, in ’e Disko to gahn.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13769</th>\n",
       "      <td>1083539</td>\n",
       "      <td>nds</td>\n",
       "      <td>Ik harr dat Geld nehmen schullt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>811082</td>\n",
       "      <td>nds</td>\n",
       "      <td>As he de Naricht hören deed, weer he verbaast.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>839586</td>\n",
       "      <td>nds</td>\n",
       "      <td>He schreet veel.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id language                                             nds\n",
       "8193    906561      nds   Uns Vörfohren kemen vör 150 Johr in dit Land.\n",
       "1387    810834      nds      Se verlöövt uns nich, in ’e Disko to gahn.\n",
       "13769  1083539      nds                Ik harr dat Geld nehmen schullt.\n",
       "1516    811082      nds  As he de Naricht hören deed, weer he verbaast.\n",
       "3472    839586      nds                                He schreet veel."
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nds_sentences.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:09.060063Z",
     "start_time": "2020-04-23T14:08:09.044249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>deu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>deu</td>\n",
       "      <td>Lass uns etwas versuchen!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78</td>\n",
       "      <td>deu</td>\n",
       "      <td>Ich muss schlafen gehen.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id language                        deu\n",
       "0  77      deu  Lass uns etwas versuchen!\n",
       "1  78      deu   Ich muss schlafen gehen."
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deu_sentences.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:09.073927Z",
     "start_time": "2020-04-23T14:08:09.064370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin  translation\n",
       "0       1         1276\n",
       "1       1         2481"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_sentences.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:12.394714Z",
     "start_time": "2020-04-23T14:08:09.077593Z"
    }
   },
   "outputs": [],
   "source": [
    "df = link_sentences.merge(deu_sentences\n",
    "                     , left_on = \"origin\"\n",
    "                     , right_on = \"id\").merge(nds_sentences\n",
    "                                              , left_on=\"translation\", right_on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:12.419713Z",
     "start_time": "2020-04-23T14:08:12.396786Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[[\"deu\", \"nds\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:12.458434Z",
     "start_time": "2020-04-23T14:08:12.425261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deu</th>\n",
       "      <th>nds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16769</th>\n",
       "      <td>Es ist schon fast sieben, wir müssen zur Schul...</td>\n",
       "      <td>Dat is dicht an söven. Wi mööt na School (gahn).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16770</th>\n",
       "      <td>Es ist schon fast sieben Uhr, wir müssen zur S...</td>\n",
       "      <td>Dat is dicht an söven. Wi mööt na School (gahn).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16771</th>\n",
       "      <td>Es ist schon fast sieben, wir müssen zur Schule.</td>\n",
       "      <td>Dat is dicht an söven. Wi mööt na School (gahn).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16894</th>\n",
       "      <td>Ich mag wxWidgets, weil es im Gegensatz zu and...</td>\n",
       "      <td>Ik mag wxWidgets vonwegen dat dat anners as an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17483</th>\n",
       "      <td>Wir rufen euch an.</td>\n",
       "      <td>Wü röp juu ön (frr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17484</th>\n",
       "      <td>Spielst du Baseball?</td>\n",
       "      <td>Spölest Baseball? (frr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17485</th>\n",
       "      <td>Sie wird es dir sagen.</td>\n",
       "      <td>Jü wel et di sii. (frr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17486</th>\n",
       "      <td>Tom brachte mir eine Tasse Tee.</td>\n",
       "      <td>Tom braacht mi en Kop Tee. (frr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17487</th>\n",
       "      <td>Wir hatten ein gutes Gespräch.</td>\n",
       "      <td>Wü her en gur Snak. (frr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17488</th>\n",
       "      <td>Wir wollten mit ihr sprechen.</td>\n",
       "      <td>Wü wil me höör snaki (frr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17489</th>\n",
       "      <td>Sami fährt nicht.</td>\n",
       "      <td>Sami köört ek (frr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17490</th>\n",
       "      <td>Er stieg von seinem Pferd.</td>\n",
       "      <td>Hi steeg of fan sin Hingst. (frr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17491</th>\n",
       "      <td>Wirst du mit ihr nach Boston gehen?</td>\n",
       "      <td>Wet me höör tö Boston gung? (frr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17492</th>\n",
       "      <td>Mein Hemd ist aus Baumwolle.</td>\n",
       "      <td>Min Sjüürt es fan Boomel (frr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17493</th>\n",
       "      <td>Mir ist Wasser ins Ohr gekommen.</td>\n",
       "      <td>Weeter es mi iin ön Uar kemen (frr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17494</th>\n",
       "      <td>Weißt du, wo meine Schlüssel sind?</td>\n",
       "      <td>Weest, hur min Kaien sen? (frr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17495</th>\n",
       "      <td>Ist dieses Metall Kupfer?</td>\n",
       "      <td>Es det Metal fan Kööper? (frr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17496</th>\n",
       "      <td>Das Schloss hatte vier schwere Türen aus Bronze.</td>\n",
       "      <td>Di Sloot her fjuur swaar Düüren fan Bronsi (frr)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     deu  \\\n",
       "16769  Es ist schon fast sieben, wir müssen zur Schul...   \n",
       "16770  Es ist schon fast sieben Uhr, wir müssen zur S...   \n",
       "16771   Es ist schon fast sieben, wir müssen zur Schule.   \n",
       "16894  Ich mag wxWidgets, weil es im Gegensatz zu and...   \n",
       "17483                                 Wir rufen euch an.   \n",
       "17484                               Spielst du Baseball?   \n",
       "17485                             Sie wird es dir sagen.   \n",
       "17486                    Tom brachte mir eine Tasse Tee.   \n",
       "17487                     Wir hatten ein gutes Gespräch.   \n",
       "17488                      Wir wollten mit ihr sprechen.   \n",
       "17489                                  Sami fährt nicht.   \n",
       "17490                         Er stieg von seinem Pferd.   \n",
       "17491                Wirst du mit ihr nach Boston gehen?   \n",
       "17492                       Mein Hemd ist aus Baumwolle.   \n",
       "17493                   Mir ist Wasser ins Ohr gekommen.   \n",
       "17494                 Weißt du, wo meine Schlüssel sind?   \n",
       "17495                          Ist dieses Metall Kupfer?   \n",
       "17496   Das Schloss hatte vier schwere Türen aus Bronze.   \n",
       "\n",
       "                                                     nds  \n",
       "16769   Dat is dicht an söven. Wi mööt na School (gahn).  \n",
       "16770   Dat is dicht an söven. Wi mööt na School (gahn).  \n",
       "16771   Dat is dicht an söven. Wi mööt na School (gahn).  \n",
       "16894  Ik mag wxWidgets vonwegen dat dat anners as an...  \n",
       "17483                                Wü röp juu ön (frr)  \n",
       "17484                            Spölest Baseball? (frr)  \n",
       "17485                            Jü wel et di sii. (frr)  \n",
       "17486                   Tom braacht mi en Kop Tee. (frr)  \n",
       "17487                          Wü her en gur Snak. (frr)  \n",
       "17488                         Wü wil me höör snaki (frr)  \n",
       "17489                                Sami köört ek (frr)  \n",
       "17490                  Hi steeg of fan sin Hingst. (frr)  \n",
       "17491                  Wet me höör tö Boston gung? (frr)  \n",
       "17492                     Min Sjüürt es fan Boomel (frr)  \n",
       "17493                Weeter es mi iin ön Uar kemen (frr)  \n",
       "17494                    Weest, hur min Kaien sen? (frr)  \n",
       "17495                     Es det Metal fan Kööper? (frr)  \n",
       "17496   Di Sloot her fjuur swaar Düüren fan Bronsi (frr)  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"nds\"].str.contains('\\(')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:12.478454Z",
     "start_time": "2020-04-23T14:08:12.460332Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(df[df[\"nds\"].str.contains('\\(frr')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:12.493310Z",
     "start_time": "2020-04-23T14:08:12.483733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deu</th>\n",
       "      <th>nds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>Ich habe dir dein Abendessen im Ofen gelassen.</td>\n",
       "      <td>Ik heff di dien Avendeten in ’n Aven laten.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16796</th>\n",
       "      <td>Wir beide wissen, dass ihr beiden lügt.</td>\n",
       "      <td>Wi beide weet dat ji beide leegt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10115</th>\n",
       "      <td>Das war sein eigener Fehler.</td>\n",
       "      <td>Dat weer sien egen Schuld.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>Warum nimmst du dir den Tag nicht frei?</td>\n",
       "      <td>Worüm nimmst du di den Dag nich free?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7807</th>\n",
       "      <td>Wir haben eine Tochter, die mit einem Franzose...</td>\n",
       "      <td>Wi hebbt en Dochter, de mit en Franzoos verhei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     deu  \\\n",
       "2132      Ich habe dir dein Abendessen im Ofen gelassen.   \n",
       "16796            Wir beide wissen, dass ihr beiden lügt.   \n",
       "10115                       Das war sein eigener Fehler.   \n",
       "3531             Warum nimmst du dir den Tag nicht frei?   \n",
       "7807   Wir haben eine Tochter, die mit einem Franzose...   \n",
       "\n",
       "                                                     nds  \n",
       "2132         Ik heff di dien Avendeten in ’n Aven laten.  \n",
       "16796                  Wi beide weet dat ji beide leegt.  \n",
       "10115                         Dat weer sien egen Schuld.  \n",
       "3531               Worüm nimmst du di den Dag nich free?  \n",
       "7807   Wi hebbt en Dochter, de mit en Franzoos verhei...  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:12.562460Z",
     "start_time": "2020-04-23T14:08:12.498587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deu</th>\n",
       "      <th>nds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>Jetzt habe ich Zeit.</td>\n",
       "      <td>Nu häb ick Tied.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>Rede du nur, ich tue, was ich will.</td>\n",
       "      <td>Küer man to, ick do wat ick wil.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>Gut, dann bedanke ich mich auch.</td>\n",
       "      <td>Na, denn bedank ick mi ok.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16138</th>\n",
       "      <td>In diesem Stadium der Weltgeschichte gibt es s...</td>\n",
       "      <td>Op dat hierste Stadium vun de Werltgeschicht g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16532</th>\n",
       "      <td>Wann soll ich kommen?</td>\n",
       "      <td>Üm wölke Klock mutt ick kamen?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16746</th>\n",
       "      <td>Hier kann ich dir nicht zustimmen.</td>\n",
       "      <td>Dorup kann ick nich mit di tostimmen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16802</th>\n",
       "      <td>Wann soll ich einchecken?</td>\n",
       "      <td>Wo lååt mutt ick inchecken?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     deu  \\\n",
       "1094                                Jetzt habe ich Zeit.   \n",
       "4949                 Rede du nur, ich tue, was ich will.   \n",
       "15997                   Gut, dann bedanke ich mich auch.   \n",
       "16138  In diesem Stadium der Weltgeschichte gibt es s...   \n",
       "16532                              Wann soll ich kommen?   \n",
       "16746                 Hier kann ich dir nicht zustimmen.   \n",
       "16802                          Wann soll ich einchecken?   \n",
       "\n",
       "                                                     nds  \n",
       "1094                                    Nu häb ick Tied.  \n",
       "4949                    Küer man to, ick do wat ick wil.  \n",
       "15997                         Na, denn bedank ick mi ok.  \n",
       "16138  Op dat hierste Stadium vun de Werltgeschicht g...  \n",
       "16532                     Üm wölke Klock mutt ick kamen?  \n",
       "16746              Dorup kann ick nich mit di tostimmen.  \n",
       "16802                        Wo lååt mutt ick inchecken?  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"nds\"].str.contains('(?i) ick')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:12.589382Z",
     "start_time": "2020-04-23T14:08:12.565691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17483\n",
      "16895\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df = df[(df.deu.str.len() < 70) & (df.nds.str.len() < 70)]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T18:55:01.873029Z",
     "start_time": "2020-04-16T18:55:01.866543Z"
    }
   },
   "source": [
    "## Wikipedia parallel sentences\n",
    "\n",
    "Facebook aligned through all languages of Wikipedia suitable sentences and published it on: https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix\n",
    "\n",
    "Luckily there is a lower german version of Wikipedia with over 60.000 articles. Of course many of them are specific for lower german topics and therefore there is no german version of the article. Still Facebook managed it to align 70.000 sentences.\n",
    "We will use this dataset to have a bigger databasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:13.062497Z",
     "start_time": "2020-04-23T14:08:12.592654Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "wiki_deu = pd.read_csv(\"data/fb-wiki/WikiMatrix.de-nds-de.txt\",\n",
    "                       sep=\"\\n+\",engine='python', encoding=\"utf-8\",\n",
    "                       header=None, names=[\"deu\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:13.797845Z",
     "start_time": "2020-04-23T14:08:13.064750Z"
    }
   },
   "outputs": [],
   "source": [
    "wiki_nds = pd.read_csv(\"data/fb-wiki/WikiMatrix.de-nds-nds.txt\"\n",
    "                       , sep=\"\\n+\",engine='python'\n",
    "                       , encoding=\"utf-8\", names=[\"nds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:13.906059Z",
     "start_time": "2020-04-23T14:08:13.800313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75590\n",
      "19083\n"
     ]
    }
   ],
   "source": [
    "wiki_df = wiki_deu.join(wiki_nds)\n",
    "print(len(wiki_df))\n",
    "# exclude long sentences for faster training\n",
    "wiki_df = wiki_df[(wiki_df.deu.str.len() < 70) & (wiki_df.nds.str.len() < 70)]\n",
    "print(len(wiki_df))\n",
    "\n",
    "df = df.append(wiki_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:13.917576Z",
     "start_time": "2020-04-23T14:08:13.910159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35978"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalizing the low german words\n",
    "\n",
    "Here we try to correct the sentences with the \"official\" spelling according to [Sass wordbook](http://sass-platt.de/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:13.924786Z",
     "start_time": "2020-04-23T14:08:13.921437Z"
    }
   },
   "outputs": [],
   "source": [
    "import docx\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The available dictionary is not ordered. But the German words are written in bold. Further German subwords are written in italic. The translated words are following the bold or the italic words. First we will try to get the index range of each translation.\n",
    "For that we need the paragraph in which the word is and the range inside the paragraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:13.933517Z",
     "start_time": "2020-04-23T14:08:13.929026Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to find only the first translation\n",
    "# easier to understand in combination when used inside the loop\n",
    "def find_first_translation(translation_id, runs):\n",
    "        translation = ''\n",
    "        first_row_of_ones = False\n",
    "        for idx,i in enumerate(translation_id):\n",
    "            if i == 1:\n",
    "                translation += runs[idx].text\n",
    "                first_row_of_ones = True\n",
    "            elif i == 0 and first_row_of_ones:\n",
    "                break\n",
    "        return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:08:13.945957Z",
     "start_time": "2020-04-23T14:08:13.937275Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_word_list(document, word_list, translation_font = \"Rockwell\"):\n",
    "    idx_col = 0\n",
    "    idx_row = 0\n",
    "    string = \"\"\n",
    "    for paragraph in document.paragraphs:\n",
    "        # grab only the lines which start with a word / other lines are additional content or page numbers\n",
    "        new_line = re.match(r'\\A[A-Za-zÄäÜüÖö]', paragraph.text)\n",
    "        if new_line:\n",
    "            # write the word into the column 0 with the format bold and not italic which stands for the basis word\n",
    "            word_list.iloc[idx_row,0] = string.join([x.text for x in paragraph.runs \n",
    "                                                               if x.bold and not x.italic])\n",
    "            # now we want to grab only the translation without catching additional content with the\n",
    "            # same font. When the Font changes, it means, that the translation is finished.\n",
    "            font_list = [1 if x.font.name == translation_font else 0 for x in paragraph.runs]\n",
    "            word_list.iloc[idx_row,1] = find_first_translation(font_list, paragraph.runs)\n",
    "            # jump to next row for next entry\n",
    "            idx_row += 1\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.590Z"
    }
   },
   "outputs": [],
   "source": [
    "# first we will load the deu-nds word_list\n",
    "\n",
    "deu_nds_raw = docx.Document(\"gathering_data/sass/hd-nd-wortliste_absatz_nach_wort.docx\")\n",
    "# prepare a word list only containing one translation\n",
    "deu_nds_word_list = pd.DataFrame(index=np.arange(20000), columns= [\"deu\",\"nds\"])\n",
    "\n",
    "# pass into function\n",
    "get_word_list(deu_nds_raw, deu_nds_word_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.593Z"
    }
   },
   "outputs": [],
   "source": [
    "# now the nds-deu word_list\n",
    "\n",
    "nds_deu_raw = docx.Document(\"gathering_data/sass/nd-hd-wortliste_absatz_nach_wort.docx\")\n",
    "# prepare a word list only containing one translation\n",
    "nds_deu_word_list = pd.DataFrame(index=np.arange(20000), columns= [\"nds\",\"deu\"])\n",
    "\n",
    "# pass into function\n",
    "get_word_list(nds_deu_raw, nds_deu_word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.596Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deu_nds_word_list.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.598Z"
    }
   },
   "outputs": [],
   "source": [
    "nds_deu_word_list.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the data now in our pandas dataframe, we can do some regex and clean the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.602Z"
    }
   },
   "outputs": [],
   "source": [
    "nds_deu_word_list[(nds_deu_word_list.deu == \"\") | (nds_deu_word_list.nds == \"\")].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.605Z"
    }
   },
   "outputs": [],
   "source": [
    "#drop nan and uncomplete rows first\n",
    "\n",
    "def drop_uncomplete(df):\n",
    "    df.dropna(inplace=True)\n",
    "    drop_index = df[(df.deu == \"\") | (df.nds == \"\")].index\n",
    "    df.drop(index=drop_index, inplace=True)\n",
    "\n",
    "drop_uncomplete(deu_nds_word_list)\n",
    "print(\"Entries DEU-NDS version:\",len(deu_nds_word_list))\n",
    "drop_uncomplete(nds_deu_word_list)\n",
    "print(\"Entries NDS-DEU version:\",len(nds_deu_word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making one wordbook\n",
    "\n",
    "We have now two tables with the reverse translation. We can see already that they have different length. Maybe this is because of the original data or because a the reading of one document was better.\n",
    "Nevertheless as we have a computer and don't have to turn pages, we need only one table.\n",
    "\n",
    "After doing some data cleaning we will drop duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.608Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets make one table out of it\n",
    "\n",
    "nds_deu = nds_deu_word_list.append(deu_nds_word_list, sort=False, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.610Z"
    }
   },
   "outputs": [],
   "source": [
    "# now we know how big our new vocabulary could be.\n",
    "# still there are many wrong signs included as we can see only in the first 10 entries\n",
    "\n",
    "string = \"Test ~Test WetterW~asser  Abgabe4 1. 2. ,,; ~weg A~weg end/ jetzt!\"\n",
    "\n",
    "def clear_string(string):\n",
    "    # delete everything after these characters\n",
    "    string = re.split(r'[\\d.,;!?/]',string)[0]\n",
    "    # delete alternative spelling as there is no easy automatic solution for taking them into the list\n",
    "    string = re.split(r'(([A-ZÖÜÄa-zäöü]|\\s)~)',string)[0]\n",
    "    return string\n",
    "\n",
    "clear_string(string)\n",
    "\n",
    "# apply the cleaning function\n",
    "nds_deu = nds_deu_word_list.applymap(lambda x: clear_string(str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.613Z"
    }
   },
   "outputs": [],
   "source": [
    "# especially in the Low German column we have a lot of -. These are mostly format problems.\n",
    "# still some belong there. We assume that the minus is right, if it occurs in both columns\n",
    "\n",
    "def sub_minus(string):\n",
    "    return re.sub(r\"-\",\"\",string)\n",
    "\n",
    "\n",
    "nds_deu.loc[~(nds_deu.nds.str.contains(\"-{1}\") & nds_deu.deu.str.contains(\"-{1}\")),:] = nds_deu.loc[~(nds_deu.nds.str.contains(\"-{1}\") & nds_deu.deu.str.contains(\"-{1}\")),:].applymap(sub_minus)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.615Z"
    }
   },
   "outputs": [],
   "source": [
    "# delete the words \"sik\", \"wat\" in Low German as this information is about the grammar\n",
    "# same in German with \"sich\", \n",
    "\n",
    "def replace_pre_words(df):\n",
    "    df.nds = df.nds.str.replace(\"(wat\\s|sik\\s)\",\"\")\n",
    "    df.deu = df.deu.str.replace(\"(was\\s|sich\\s)\",\"\")\n",
    "\n",
    "    \n",
    "replace_pre_words(nds_deu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.618Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# drop paranthesis including the content as they are as well an additional explanation we don't want to have\n",
    "def drop_parenthesis_dash(string):\n",
    "    string = re.sub(r'\\([a-zöäü.~]*\\)?',\"\",string)\n",
    "    string = re.sub(r'\\|',\"\",string)\n",
    "    #empty spaces at the beginning or end\n",
    "    string = re.sub(r'^\\s|\\s$',\"\",string)\n",
    "    return string\n",
    "\n",
    "nds_deu = nds_deu.applymap(drop_parenthesis_dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.620Z"
    }
   },
   "outputs": [],
   "source": [
    "# In some rows there is only one letter or nothing, so a further read in error.\n",
    "# We will drop these lines\n",
    "# moreover in Low German there are some words with two characters but as well many read in errors\n",
    "def drop_read_in_errors(df):\n",
    "    drop_index = df[(df.deu.str.len() <= 1 )|(df.nds.str.len() <= 1 )].index\n",
    "    df.drop(index=drop_index,inplace=True)\n",
    "    drop_different_lengths_one_index = df[(df.nds.str.len() <= 2) & (df.deu.str.len() > 4)].index\n",
    "    df.drop(index=drop_different_lengths_one_index,inplace=True)\n",
    "    drop_different_lengths_two_index = df[(df.nds.str.len() <= 3) & (df.deu.str.len() > 7)].index\n",
    "    df.drop(index=drop_different_lengths_two_index,inplace=True)\n",
    "\n",
    "    drop_different_lengths_reverse_index = df[(df.deu.str.len() == 2) & (df.nds.str.len() > 4)].index\n",
    "    df.drop(index=drop_different_lengths_reverse_index,inplace=True)\n",
    "\n",
    "    # there are as well some read in problems with german article \"der\"\n",
    "    # we will insert it later in the additional text\n",
    "    drop_der = df[df.deu == \"der\"].index\n",
    "    df.drop(index=drop_der,inplace=True)\n",
    "    \n",
    "    \n",
    "drop_read_in_errors(nds_deu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.623Z"
    }
   },
   "outputs": [],
   "source": [
    "# load in some additonal hand picked cleaned data and append\n",
    "\n",
    "hand_picked_words = pd.read_csv(\"gathering_data/sass/additional_words.txt\", sep=\"\\t\", header=None, names = [\"nds\",\"deu\"] )\n",
    "nds_deu = nds_deu.append(hand_picked_words, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.625Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# and drop only exact duplicates as a Low German word could have several meanings in German and vice versa\n",
    "print(\"Entries before dropping duplicates: \", len(nds_deu))\n",
    "nds_deu.drop_duplicates(inplace=True)\n",
    "print(\"Entries after dropping duplicates: \", len(nds_deu))\n",
    "\n",
    "# as we did some cleaning, maybe we have created some addtional empty elements\n",
    "# we are using our drop_uncomplete function again\n",
    "\n",
    "drop_uncomplete(nds_deu)\n",
    "\n",
    "print(\"Entries after dropping empty entries: \", len(nds_deu))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.628Z"
    }
   },
   "outputs": [],
   "source": [
    "# resetting index\n",
    "nds_deu.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.630Z"
    }
   },
   "outputs": [],
   "source": [
    "nds_deu.to_csv(\"data/nds_deu_dictionary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the dictionairy\n",
    "\n",
    "Finally we have the dictionairy and can try to correct the sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T10:59:03.222037Z",
     "start_time": "2020-04-23T10:59:03.217829Z"
    }
   },
   "source": [
    "#### Now some Markdown cells which should be code, but take too long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T08:17:40.522747Z",
     "start_time": "2020-04-23T08:12:47.891261Z"
    }
   },
   "source": [
    "###### count how often words from our dictionary occur in the text already\n",
    "if 'count' not in globals():\n",
    "\n",
    "    count = [df.nds.str.count(re.escape(text)).sum() for text in nds_deu.nds]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T08:17:40.540052Z",
     "start_time": "2020-04-23T08:17:40.525596Z"
    }
   },
   "source": [
    "#[nds_deu.nds.str.count(re.escape(sentence)).sum() for sentence in df.nds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T08:17:40.593423Z",
     "start_time": "2020-04-23T08:17:40.542559Z"
    }
   },
   "source": [
    "nds_deu[\"corpus_count\"] = count\n",
    "\n",
    "\n",
    "print(\"Words not used: \", sum(nds_deu[\"corpus_count\"] == 0))\n",
    "print(\"Words used: \", len(nds_deu) - sum(nds_deu[\"corpus_count\"] == 0))\n",
    "\n",
    "nds_deu.sort_values(\"corpus_count\",ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.635Z"
    }
   },
   "outputs": [],
   "source": [
    "nds_deu.nds.str.len()[8441]\n",
    "\n",
    "nds_deu[nds_deu.nds ==\"ik\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T15:35:27.739034Z",
     "start_time": "2020-04-21T15:35:27.434281Z"
    }
   },
   "source": [
    "We don't have a frequency vector for Low German, but it is very closely related to German.\n",
    "Therefore we can use the German word_frequency for Low German as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.638Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordfreq import word_frequency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.644Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_frq(text):\n",
    "    return word_frequency(text, \"de\")\n",
    "word_frq = nds_deu.deu.apply(get_frq).rename(\"frq\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.649Z"
    }
   },
   "outputs": [],
   "source": [
    "nds_dic = nds_deu.join(word_frq)\n",
    "\n",
    "nds_dic.dropna( inplace=True)\n",
    "nds_dic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.653Z"
    }
   },
   "outputs": [],
   "source": [
    "nds_dic.sort_values(\"frq\", ascending = False).head(5)\n",
    "\n",
    "nds_dic.isna().sum()\n",
    "nds_dic[nds_dic.nds == \"ik\"]\n",
    "nds_dic[nds_dic.duplicated(\"nds\", keep=False)].head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.661Z"
    }
   },
   "outputs": [],
   "source": [
    "nds_dic = nds_dic[[\"nds\",\"deu\",\"frq\"]]\n",
    "nds_dic.reset_index(drop=True, inplace=True)\n",
    "print(\"Before removing duplicates: \",len(nds_dic))\n",
    "\n",
    "nds_dic = nds_dic.sort_values(\"frq\", ascending = True)\n",
    "nds_dic = nds_dic.drop_duplicates(subset='nds', keep=\"last\")\n",
    "print(\"After removing duplicates: \",len(nds_dic))\n",
    "nds_dic.to_csv(\"data/nds_deu_count.csv\", index=False)\n",
    "nds_dic.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.664Z"
    }
   },
   "outputs": [],
   "source": [
    "# to get full integers\n",
    "nds_dic.frq = nds_dic.frq * 100000000\n",
    "nds_dic.frq = nds_dic.frq.astype(\"int\")\n",
    "nds_tojson = nds_dic.set_index(\"nds\")\n",
    "nds_tojson = nds_tojson.to_dict(orient=\"dict\")[\"frq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.668Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/nds_dic.txt', 'w') as fp:\n",
    "    json.dump(nds_tojson, fp, ensure_ascii=False, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are loading the module Spellchecker which can correct words based on our created dictionary.\n",
    "After that we are going to see how many words from our sentences are in the dictionairy and try to automize the correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.671Z"
    }
   },
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker(local_dictionary=\"data/nds_dic.txt\")  # loads nds word frequency list\n",
    "\n",
    "spell.export(\"data/example_dict\", gzipped=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.675Z"
    }
   },
   "outputs": [],
   "source": [
    "word = \"maol\"\n",
    "spell.correction(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.679Z"
    }
   },
   "outputs": [],
   "source": [
    "spell.correction(\"ick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.682Z"
    }
   },
   "outputs": [],
   "source": [
    "spell.known([\"ik\", \"gehe\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will tokenize the words, so we can work with single words.\n",
    "Probably a simple tokenizer which splits by whitespace and punctuation would be enough, but we will use Spacy, so we will have the chance to get better tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.685Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "spacy_de = spacy.load('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.688Z"
    }
   },
   "outputs": [],
   "source": [
    "spell.known([\"ik\", \"verdenen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.691Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_spell(series, dic):\n",
    "    series = series.str.replace(r\"[\\d?!.\\\",]*\",\"\")\n",
    "    #tokenize\n",
    "    sent_tok = series.apply(spacy_de.tokenizer)\n",
    "    # get as list\n",
    "    complete = sent_tok.apply(lambda x: [a.text for a in x])\n",
    "    # get unknown and known words\n",
    "    unknown_words = complete.apply(dic.unknown)\n",
    "    known_words = complete.apply(dic.known)\n",
    "    return unknown_words, known_words, complete\n",
    "def hit_quote(series, dic):\n",
    "    unknown_words,known_words,complete = get_spell(series,dic)\n",
    "    total_words = complete.apply(len).sum()\n",
    "    # each row is saved as spell_object, therefore we have to do the following workaround\n",
    "    known_count = known_words.apply(lambda x: [1 for i in x]).apply(len).sum()\n",
    "    unknown_count = unknown_words.apply(lambda x: [1 for i in x]).apply(len).sum()\n",
    "    print(\"Total words: \", total_words)\n",
    "    print(\"Known words: \", known_count)\n",
    "    print(\"Unknown words: \", unknown_count)\n",
    "    print(\"Hit quote: \", round(100*known_count/total_words))\n",
    "\n",
    "hit_quote(df.nds, spell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex correction\n",
    "\n",
    "We have many different spellings, but for some common words or patterns, we will define the \"right\" spelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.694Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace \"ik\" with \"ick\"\n",
    "def replace_ik(df):    \n",
    "    print(df.nds.str.count(r\"(I|i)ck\").sum())\n",
    "    df.nds = df.nds.str.replace(\"(I|i)ck\", \"\\1k\")\n",
    "\n",
    "# replace us with uns\n",
    "def replace_uns(df):\n",
    "    df.nds = df.nds.str.replace(\"\\s(U|u)s\\s\", \"\\1ns\")\n",
    "\n",
    "\n",
    "# \"sch\" before a consonant will be replaced with s \n",
    "def replace_s(df):\n",
    "    print(df.nds.str.count(r\"\\s(S|s)ch[lmknbwv][a-zäöü]*\").sum())\n",
    "    index = df[df.nds.str.contains(r\"(\\s((S|s)ch)([lmknbwvptb]))\")].index\n",
    "    df.nds.str.replace(r\"((S|s)ch)([lmknbwvptb])\", r\"\\2\\3\")[index]\n",
    "    \n",
    "def regex_all(df):\n",
    "    replace_ik(df)\n",
    "    replace_uns(df)\n",
    "    replace_s(df)\n",
    "\n",
    "regex_all(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepairing sentences\n",
    "\n",
    "In this section we want to select and preprocess the sentences for our model.\n",
    "\n",
    "As explained the sentences are not perfectly aligned. To find only high-quality sentences we use our wordbook and see in which we find translations with our wordbook. Actually, this is not the perfect way as we might exclude translations, which can't be translated word by word.\n",
    "We will evaluate later if we have a gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.696Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "unknown_nds , known_nds , _ = get_spell(df.nds,spell)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.698Z"
    }
   },
   "outputs": [],
   "source": [
    "for word in known_nds[28]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.701Z"
    }
   },
   "outputs": [],
   "source": [
    "dic = pd.read_csv(\"data/nds_deu_count.csv\")\n",
    "print(len(dic))\n",
    "\n",
    "dic = dic.drop_duplicates(subset='nds', keep=\"last\")\n",
    "dic[dic.nds == \"ik \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.703Z"
    }
   },
   "outputs": [],
   "source": [
    "def german_word(dic_list):\n",
    "    strings = [dic[a]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.705Z"
    }
   },
   "outputs": [],
   "source": [
    "known_nds.apply(lambda x: dic[a] for a in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.712Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df.nds.str.contains(r\"[lpgtrda]+aa[tzrsdfgbnm]*\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.716Z"
    }
   },
   "outputs": [],
   "source": [
    "hit_quote(df.nds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.718Z"
    }
   },
   "outputs": [],
   "source": [
    "test_str = \"De Stevel is Biebel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.723Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# According to Arbatzat \"Basiswortschatz Plattdeutsch\" if it is possible, b should be replaced with v\n",
    "#assumption from my side: if there is a vowel in front and after\n",
    "\n",
    "df.nds = df.nds.str.replace(r\"([AEOUÄÜÖaeouäöü])b([AEUOaeouäöü])\", r\"\\1v\\2\")\n",
    "\n",
    "# by chance I saw that Bibel is spelled wrong a lot of times\n",
    "\n",
    "df.nds = df.nds.str.replace(r\"Bi(e|)bel\", \"Bivel\")\n",
    "\n",
    "# replacement of \"sch\" to \"s\"\n",
    "\n",
    "df.nds = df.nds.str.replace(r\"((sch)([lmn]))\", r\"s\\2\")\n",
    "df.nds = df.nds.str.replace(r\"((Sch)([lmn]))\", r\"S\\2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.727Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf=\"data/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-23T14:08:04.731Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_data, valid_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "train_data.to_csv(path_or_buf=\"data/train_data.csv\", index=False)\n",
    "valid_data.to_csv(path_or_buf=\"data/valid_data.csv\", index=False)\n",
    "test_data.to_csv(path_or_buf=\"data/test_data.csv\", index=False)\n",
    "\n",
    "print(\"Numbers of training samples: \" , len(train_data))\n",
    "print(\"Number of validation samples: \",len(valid_data))\n",
    "print(\"Number of test samples: \",len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
